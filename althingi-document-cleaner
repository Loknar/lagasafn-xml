#!/usr/bin/env python
# althingi-document-cleaner
# Version 0.1

from bs4 import BeautifulSoup
import codecs
import urllib
import re

import os
import sys

from sys import stderr
from sys import stdout

LAW_FILENAME = 'current/%d%s.html' # % (law_year, law_num)
CLEAN_FILENAME = 'cleaned/%d-%d.html' # % (law_year, law_num)

def clean_content(content):
    # Decode ISO-8859-1 character encoding.
    #content = content.decode('ISO-8859-1')

    # Replace HTML-specific space for normal space.
    content = content.replace('&nbsp;', ' ')

    # Make sure that horizontal bar tags are closed properly.
    content = content.replace('<hr>', '<hr />')

    # Make sure that linebreak tags are closed properly.
    content = content.replace('<br>', '<br />')

    # Remove markers for previous changes
    content = content.replace('[', '').replace(']', '')

    # Remove links to website
    strings_to_remove = (
        u'Ferill málsins á Alþingi.',
        u'Frumvarp til laga.',
    )
    for s in strings_to_remove:
        content = content.replace(s, '')

    # Make sure that image tags are closed properly.
    e = re.compile(r'<img ([^>]*)>', re.IGNORECASE)
    content = e.sub(r'<img \1 />', content)

    # Remove <a id=""> tags which are unclosed and seem without purpose.
    # For example, see source of page: http://www.althingi.is/altext/143/s/0470.html
    content = content.replace('<a id="">', '')

    # Find the law content and exit if it does not exist
    soup_law = BeautifulSoup(content, 'html5lib').find('html')
    if soup_law is None:
        return None

    soup = BeautifulSoup(soup_law.__str__(), 'html5lib') # Parse the law content

    # Remove superscripts indicating previous change
    superscripts = soup.find_all('sup')
    for s in superscripts:
        if re.match('^\d{1,3}\)$', s.text):
            s.extract()

    # Remove links
    anchors = soup.find_all('a')
    for a in anchors:
        a.replace_with(a.text)

    # Remove tags entirely irrelevant to content
    tags_to_remove = ['small', 'hr'] # Previously also ['script', 'noscript', 'head']
    for target_tag in tags_to_remove:
        [s.extract() for s in soup(target_tag)]

    # Remove empty tags, but only if they're empty
    empty_tags_to_remove = ['p', 'h2', 'i']
    for target_tag in empty_tags_to_remove:
        empty_tags = soup.find_all(lambda tag: tag.name == target_tag and not tag.contents and (
            tag.string is None or not tag.string.strip()
        ))
        [empty_tag.extract() for empty_tag in empty_tags]

    # Keep consecutive <br />s only at 2 at most
    brs_in_a_row = 0
    all_tags = soup.find_all()
    for t in all_tags:
        if t.name == 'br':
            if brs_in_a_row >= 2:
                t.extract()
            else:
                brs_in_a_row = brs_in_a_row + 1
        else:
            brs_in_a_row = 0

    # Replace <html> and <body> tags' with <div>s.
    '''
    body_tag = soup.find('body')
    if body_tag is not None:
        body_tag.attrs['id'] = 'body_tag'
        body_tag.name = 'div'
    html_tag = soup.find('html')
    html_tag.attrs['id'] = 'html_tag'
    html_tag.name = 'div'
    '''

    # Add charset tag
    charset_tag = soup.new_tag('meta', charset='utf-8')
    soup.insert(0, charset_tag)

    xhtml = soup.prettify()

    # Final cleanup at text-stage.
    xhtml = xhtml.replace(' <!-- Tab -->\n  ', '&nbsp;&nbsp;&nbsp;&nbsp;')

    return xhtml


def clean_law(law_num, law_year):
    with codecs.open(LAW_FILENAME % (law_year, str(law_num).zfill(3)), 'r', 'ISO-8859-1') as infile:
        raw_content = infile.read()
        infile.close()

    content = clean_content(raw_content)

    if content is None:
        stdout.write(' failed.\n')
        stderr.write('Error: Law %d/%d does not seem to exist\n' % (law_year, law_num))
        quit(1)

    if not os.path.isdir(os.path.dirname(CLEAN_FILENAME)):
        os.mkdir(os.path.dirname(CLEAN_FILENAME))

    with open(CLEAN_FILENAME % (law_year, law_num), 'w') as clean_file:
        clean_file.write(content)
        clean_file.close()


def process_law(law_id):

    stdout.write('Processing law nr. %s...' % law_id)
    stdout.flush()

    (law_num, law_year) = law_id.split('/')
    law_num = int(law_num)
    law_year = int(law_year)

    stdout.write('cleaning...')
    stdout.flush()
    clean_law(law_num, law_year)

    stdout.write(' done\n')


def usage(exec_name, message=None):
    stderr.write('Usage: %s [law_number>/<year>] [year] [-a]\n' % exec_name)
    stderr.write('\n')
    stderr.write('Options:\n')
    stderr.write('    -a    Process all available laws.\n')
    stderr.write('\n')
    if message:
        stderr.write('Error: %s\n' % message)
    quit(1)


def main(argv):

    law_ids = set()
    for arg in argv:
        if arg == '-a':
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^\d{7}\.html$', filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{4}$', arg):
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^%s\d{3}\.html$' % arg, filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{1,4}\/\d{4}$', arg):
            law_ids.add(arg)

    if len(law_ids) == 0:
        usage(argv[0], 'No files matched the given criteria.')

    for law_id in law_ids:
        process_law(law_id)


try:
    main(sys.argv)
except KeyboardInterrupt:
    quit()
except Exception as e:
    stderr.write('Error: %s\n' % e)

