#!/usr/bin/env python
# althingi-document-cleaner
# Version 0.1

from bs4 import BeautifulSoup
import codecs
import urllib
import re
import roman

import os
import sys

from copy import copy
from lxml import etree
from lxml.builder import E
from sys import stderr
from sys import stdout

DEBUG = True

CURRENT_PARLIAMENT_VERSION = '148a'

LAW_FILENAME = CURRENT_PARLIAMENT_VERSION + '/%d%s.html' # % (law_year, law_num)
CLEAN_FILENAME = 'cleaned/%d-%d.html' # % (law_year, law_num)
XML_FILENAME = 'xml/%d.%d.%s.xml' # % (law_year, law_num, parliament_version)

def clean_content(content):
    # Decode ISO-8859-1 character encoding.
    #content = content.decode('ISO-8859-1')

    # Replace HTML-specific space for normal space.
    content = content.replace('&nbsp;', ' ')

    # Make sure that horizontal bar tags are closed properly.
    content = content.replace('<hr>', '<hr />')

    # Make sure that linebreak tags are closed properly.
    content = content.replace('<br>', '<br />')

    # Remove markers for previous changes
    content = content.replace('[', '').replace(']', '')

    # Remove links to website
    #strings_to_remove = (
    #    u'Ferill málsins á Alþingi.',
    #    u'Frumvarp til laga.',
    #)
    #for s in strings_to_remove:
    #    content = content.replace(s, '')

    # Make sure that image tags are closed properly.
    e = re.compile(r'<img ([^>]*)>', re.IGNORECASE)
    content = e.sub(r'<img \1 />', content)

    # Remove <a id=""> tags which are unclosed and seem without purpose.
    # For example, see source of page: http://www.althingi.is/altext/143/s/0470.html
    content = content.replace('<a id="">', '')

    # Find the law content and exit if it does not exist
    soup_law = BeautifulSoup(content, 'html5lib').find('html')
    if soup_law is None:
        return None

    soup = BeautifulSoup(soup_law.__str__(), 'html5lib') # Parse the law content

    # Remove superscripts indicating previous change
    superscripts = soup.find_all('sup')
    for s in superscripts:
        if re.match('^\d{1,3}\)$', s.text):
            s.extract()

    # Remove links
    #anchors = soup.find_all('a')
    #for a in anchors:
    #    a.replace_with(a.text)

    # Remove tags entirely irrelevant to content
    tags_to_remove = ['small'] # Previously also ['hr', 'script', 'noscript', 'head']
    for target_tag in tags_to_remove:
        [s.extract() for s in soup(target_tag)]

    # Remove empty tags, but only if they're empty
    empty_tags_to_remove = ['p', 'h2', 'i']
    for target_tag in empty_tags_to_remove:
        empty_tags = soup.find_all(lambda tag: tag.name == target_tag and not tag.contents and (
            tag.string is None or not tag.string.strip()
        ))
        [empty_tag.extract() for empty_tag in empty_tags]

    # Keep consecutive <br />s only at 2 at most
    brs_in_a_row = 0
    all_tags = soup.find_all()
    for t in all_tags:
        if t.name == 'br':
            if brs_in_a_row >= 2:
                t.extract()
            else:
                brs_in_a_row = brs_in_a_row + 1
        else:
            brs_in_a_row = 0

    # Replace <html> and <body> tags' with <div>s.
    '''
    body_tag = soup.find('body')
    if body_tag is not None:
        body_tag.attrs['id'] = 'body_tag'
        body_tag.name = 'div'
    html_tag = soup.find('html')
    html_tag.attrs['id'] = 'html_tag'
    html_tag.name = 'div'
    '''

    # Add charset tag
    charset_tag = soup.new_tag('meta', charset='utf-8')
    soup.insert(0, charset_tag)

    xhtml = soup.prettify()

    # Final cleanup at text-stage.
    xhtml = xhtml.replace(' <!-- Tab -->\n  ', '&nbsp;&nbsp;&nbsp;&nbsp;')

    return xhtml


def clean_law(law_num, law_year):
    with codecs.open(LAW_FILENAME % (law_year, str(law_num).zfill(3)), 'r', 'ISO-8859-1') as infile:
        raw_content = infile.read()
        infile.close()

    content = clean_content(raw_content)

    if content is None:
        stdout.write(' failed.\n')
        stderr.write('Error: Law %d/%d does not seem to exist\n' % (law_year, law_num))
        quit(1)

    if not os.path.isdir(os.path.dirname(CLEAN_FILENAME)):
        os.mkdir(os.path.dirname(CLEAN_FILENAME))

    with open(CLEAN_FILENAME % (law_year, law_num), 'w') as clean_file:
        clean_file.write(content)
        clean_file.close()


def make_xml(law_num, law_year, parliament_version):

    # A super-iterator for containing all sorts of extra functionality that we
    # dont' get with a regular Python iterator. Note that this thing is
    # incompatible with yields and is NOT a subclass of `iter` (since that's
    # not possible), but rather a class trying its best to masquerade as one.
    #
    # TODO: See if the collection functions below rather belong in here.
    class super_iter():
        def __init__(self, collection):
            self.collection = collection
            self.index = 0

        def __next__(self):
            try:
                result = self.collection[self.index]
                self.index += 1
            except IndexError:
                raise StopIteration
            return result

        '''
        def prev(self):
            self.index -= 1
            if self.index < 0:
                raise StopIteration
            return self.collection[self.index]
        '''

        def __iter__(self):
            return self

        # Peek into the next item of the iterator without advancing it.
        # Works with negative numbers to take a peek at previous items.
        def peek(self, number_of_lines=1):
            peek_index = self.index - 1 + number_of_lines
            if peek_index >= len(self.collection) or peek_index < 0:
                return None
            return self.collection[peek_index]

    # Make sure that the XML output directory exists.
    if not os.path.isdir(os.path.dirname(XML_FILENAME)):
        os.mkdir(os.path.dirname(XML_FILENAME))

    # Construct the output XML object.
    law = E.law('', {'nr': str(law_num), 'year': str(law_year), 'parliament': parliament_version})

    # Open and read the cleaned HTML version of the law.
    with open(CLEAN_FILENAME % (law_year, law_num)) as clean_file:
        lines = super_iter(clean_file.readlines())
        clean_file.close()

    # Keeps track of the turn of events. We can query this list to check for
    # example whether the name of the document has been processed, or what the
    # last thing to be processed was. This gives us context when determining
    # what to do next.
    trail = ['start']

    # A helper class to be able to check if a regex matches in an
    # if-statement, but then process the results in its body, if there's a
    # match. This is essentially to make up for Python's (consciously decided)
    # inability to assign values to variables inside if-statements. Note that
    # a single instance of it is created and then used repeatedly.
    #
    # Usage:
    #
    # if matcher.check(line, '<tag goo="(\d+)" splah="(\d+)">'):
    #     goo, splah = matcher.result()
    #
    class Matcher():
        match = None
        def check(self, line, test_string):
            self.match = re.match(test_string, line)
            return self.match != None

        def result(self):
            return self.match.groups()
    matcher = Matcher()

    # Functions for collecting heaps of text, then returning it all in one go
    # and resetting the collection for the next time we need to collect a
    # bunch of text.
    def collect(string):
        if not hasattr(collect, 'collection'):
            collect.collection = []
        collect.collection.append(string.strip())
    def uncollect():
        result = ' '.join(collect.collection).strip()
        collect.collection = []
        return result

    # Will collect lines until the given string is found, and then return the
    # collected lines.
    def collect_until(lines, end_string):
        done = False
        while not done:
            line = next(lines).strip()
            if matcher.check(line, end_string):
                done = True
                continue
            collect(line)

        total = uncollect().strip()

        return total

    # Scrolls the lines until the given string is found. It works internally
    # the same as the collect_until-function, but is provided here with a
    # different name to provide a semantic distinction in the code below.
    scroll_until = collect_until

    # Iterators for keeping track of the order of things.
    chapter_i = 0

    # Objects that help us figure out the current state of affairs. These
    # variables are used between iterations, meaning that whenever possible,
    # their values should make sense at the end of the processing of a
    # particular line or clause. Never put nonsense into them because it will
    # completely confuse the processing elsewhere.
    chapter = None
    art = None
    subart = None

    # The cleaned document that we're processing is expected to put every tag,
    # both its opening and closing, on a separate line. This allows us to
    # browse the HTML contents on a per-line basis.
    for line in lines:
        line = line.strip()

        if line == '<h2>':
            # Parse law name.
            name = collect_until(lines, '</h2>')

            law.append(E.name(name))

            trail.append('name')

        elif line == '<strong>':
            if trail[-1] == 'name':
                # Parse the num and date, which appears directly below the law name.
                num_and_date = collect_until(lines, '</strong>')

                law.append(E('num-and-date', num_and_date))

                trail.append('num-and-date')

        elif line == '<hr/>':
            if trail[-1] == 'num-and-date':
                # Parse the whole clause about which minister the law refers
                # to. It contains HTML goo, but we'll just let it float along.
                # It's not even certain that we'll be using it, but there's no
                # harm in keeping it around.
                minister_clause = collect_until(lines, '<hr/>')

                law.append(E('minister-clause', minister_clause))

                trail.append('minister-clause')

        elif line == '<b>':
            # Chapters are found by finding a <b> tag that comes right after a
            # <br/> tag that occurs after the ministerial clause is done.
            if 'minister-clause' in trail and lines.peek(-1).strip() == '<br/>':
                # Parse what we will believe to be a chapter.

                # Chapter names are a bit tricky. They may be divided into two
                # <b> clauses, in which case the former one is what we call a
                # nr-title (I. kafli, II. kafli etc.), and then a name
                # describing its content (Almenn ákvæði, Tilgangur og markmið
                # etc.).
                #
                # Sometimes however, there is only one <b> and not two. In
                # these cases, there is no nr-title and only a name. Here we
                # check 3 lines into the future to see if there's another <b>
                # tag coming (because we know that the former <b>text</b>
                # clause will be precisely 3 lines), and if so, we'll process
                # the nr-title and name, but otherwise we will only process
                # the name and skip nr-title altogether.
                if lines.peek(3).strip() == '<b>':
                    chapter_nr_title = collect_until(lines, '</b>')

                    scroll_until(lines, '<b>')
                    chapter_name = collect_until(lines, '</b>')

                    chapter_i += 1
                    chapter = E.chapter(
                        {'nr': str(chapter_i)},
                        E('nr-title', chapter_nr_title),
                        E('name', chapter_name)
                    )

                else:
                    chapter_name = collect_until(lines, '</b>')

                    chapter = E.chapter(
                        E('name', chapter_name)
                    )

                # Some laws have a chapter for temporary clauses, which may be
                # named something like "Bráðabirgðaákvæði", "Ákvæði til
                # bráðabirgða" and probably something else as well. We will
                # assume that a chapter name that includes that string
                # "bráðabirgð" is a chapter for temporary clauses and
                # furthermore that the string does NOT appear in any other
                # kind of chapter.
                if 'bráðabirgð' in chapter_name:
                    chapter.attrib['type'] = 'temporary-clauses'

                law.append(chapter)

                trail.append('chapter')

        elif line == '<img alt="" height="11" src="/lagas/sk.jpg" width="11"/>':
            # Parse an article.
            scroll_until(lines, '<b>')
            art_nr_title = collect_until(lines, '</b>')

            # Hopefully this stays None. Not because anything will break
            # otherwise, but because Roman numerals suck ass.
            art_roman_nr = None

            # Analyze the displayed article name.
            try:
                # A typical article is called something like "13. gr." or
                # "13. gr. b". The common theme among typical articles is that
                # the string ". gr." will appear in them somewhere. Anything
                # before it is the article number. Anything after it, is an
                # extra thing which is appended to article names when new
                # articles are placed between two existing ones. For example,
                # if there already are articles 13 and 14 but the legislature
                # believes that a new article properly belongs between them,
                # the article will probably be called "13. gr. a". We would
                # want that translated into an sortable `art_nr` f.e. "13a".

                # Find index of common string. If the string does not exist,
                # it's some sort of a special article. A ValueError will be
                # raised and we'll deal with it below.
                gr_index = art_nr_title.index('. gr.')

                # Determine the numeric part of the article's name.
                art_nr = art_nr_title[0:gr_index]

                # Check if there is an extra part to the name which we'll want
                # appended to the `art_nr`.
                #
                # Note: len('.gr.') + 1 = 6
                art_nr_extra = art_nr_title[gr_index+6:].strip('.')
                if len(art_nr_extra):
                    art_nr = '%s%s' % (art_nr, art_nr_extra)

            except ValueError:
                # This means that the article's name is formatted in an
                # unconventional way. Typically this occurs only in temporary
                # clauses that tend to be numbered with some stupid fucking
                # Roman bullshit.
                art_nr = None
                try:
                    art_roman_nr = str(roman.fromRoman(art_nr_title.strip('.')))
                except roman.InvalidRomanNumeralError:
                    # Good. Fuck Roman numerals.
                    pass

            # Create the tags, configure them and append to the chapter.
            art = E('art', E('nr-title', art_nr_title))
            if art_nr is not None:
                art.attrib['nr'] = art_nr
            if art_roman_nr is not None:
                art.attrib['roman-nr'] = art_roman_nr
                art.attrib['number-type'] = 'roman'
            chapter.append(art)

            # Check if the next line is an <em>, because if so, then the
            # article has a name title and we need to grab it. Note that not
            # all articles have names.
            if lines.peek().strip() == '<em>':
                scroll_until(lines, '<em>')
                art_name = collect_until(lines, '</em>')

                art.append(E('name', art_name))

            trail.append('art-nr')

            # There can be no current subarticle if we've just discovered a
            # new article.
            subart = None

        elif matcher.check(line, '<img alt="" height="11" id="G(\d+)M(\d+)" src="\/lagas\/hk.jpg" width="11"\/>'):
            art_nr, subart_nr = matcher.result()

            # TODO: This content needs to be separated down further, but it's
            # a bit of an independent science that we will deal with
            # independently.
            content = collect_until(lines, '<br/>')

            subart = E('subart', {'nr': subart_nr }, E('sen', content))
            art.append(subart)

            trail.append('subart')

        elif matcher.check(line, '<span id="G(\d+)([0-9A-Z]*)L(\d+)">'):
            # fake_numart_nr is named so because it doesn't give the current
            # numart_nr. Not only is it always a digit, when in fact a real
            # numart_nr can be alphabetical, but even if we only had numbers,
            # they would still be wrong in some recursive lists. This is why
            # we won't trust it, and will rather analyze the content of the
            # next line, which will contain the correct numart_nr in the
            # correct type.
            numart_nr = lines.peek().strip().strip('.')

            # Dictates where we will place this numart.
            parent = None

            if trail[-1] == 'numart':
                # `prev_numart` is only defined here for readability. `numart` hasn't
                # been set yet, so it's still equivalent to the previous
                # `numart`, so technically we could just use `numart` instead.
                # But `numart` is going to change later, so we'd rather be
                # clear on the code semantics when using `numart` but still
                # referring to the previous one.
                prev_numart = numart

                prev_numart_nr = prev_numart.attrib['nr']

                # Check if the type of the numart has changed from numeric to
                # alphabetic or vice versa.
                different_type = any([
                    prev_numart_nr.isdigit() and not numart_nr.isdigit(),
                    not prev_numart_nr.isdigit() and numart_nr.isdigit()
                ])

                if prev_numart_nr.isdigit():
                    expected_numart_nr = str(int(prev_numart_nr) + 1)
                else:
                    expected_numart_nr = chr(int(ord(prev_numart_nr)) + 1)

                if expected_numart_nr != numart_nr or different_type:
                    if numart_nr == 'a' or (numart_nr.isdigit() and int(numart_nr) == 1):
                        # A new list has started within the one we were
                        # already processing, which we can tell because there
                        # was a `numart` before this one, but this `numart`
                        # says it's at the beginning, being either 'a' or 1.
                        # In this case, we'll choose the previous `numart` as
                        # the parent, so that this new list will be inside the
                        # previous one.
                        parent = prev_numart
                    else:
                        # A different list is being handled now, but it's not
                        # starting at the beginning (is neither 'a' nor 1).
                        # This means that we've been dealing with a sub-list
                        # which has now finished, so we want to continue
                        # appending this `numart` to the parent of the parent
                        # of the list we've been working on recently, which is
                        # the same parent as the nodes that came before we
                        # started the sub-list.
                        parent = prev_numart.getparent().getparent()
                else:
                    # This `numart` is simply the next one, so we'll want to
                    # append it to whatever node that the previous `numart`
                    # was appended to.
                    parent = prev_numart.getparent()

            # A parent may already be set above if `numart` currently being
            # handled is not the first one in its parent article/subarticle.
            # However, if it is indeed the first one, we need to figure out
            # where to place it. It can be placed wither in a subarticle or in
            # some cases, directly into an article without a subarticle.
            if parent is None:
                if subart is not None:
                    parent = subart
                else:
                    # We sure as $h17 assume "art" exists.
                    parent = art

            # Create numerical article.
            numart = E('numart', {'nr': numart_nr })

            # Add the numerical article to its parent.
            parent.append(numart)

            numart_nr_title = collect_until(lines, '</span>')
            numart.append(E('nr-title', numart_nr_title))
            if not numart_nr_title.strip('.').isdigit():
                numart.attrib.update({'type': 'alphabet'})

            if lines.peek().strip() == '<i>':
                # Looks like this numerical article has a name.
                scroll_until(lines, '<i>')
                numart_name = collect_until(lines, '</i>')

                numart.append(E('name', numart_name))

            content = collect_until(lines, '<br/>')

            numart.append(E('sen', content))

            trail.append('numart')


    # Write the XML object to output file.
    with open(XML_FILENAME % (law_year, law_num, parliament_version), 'w') as f:
        # Importing a completely different XML library than the one we're
        # using elsewhere in the code is a bit weird, but this is the only one
        # we could find that does pretty printing with proper indenting. That
        # happens to be very important for seeing whether the end result
        # works. Since it's only used when DEBUG=True and is very much an
        # anomaly in the code, it is imported here instead of at the top of
        # the file.
        #
        # When not in DEBUG mode, we'll skip those shenanigans and write it
        # out with the same library as the one we use elsewhere.
        if DEBUG:
            import xml.dom.minidom
            xml = xml.dom.minidom.parseString(etree.tostring(
                    law,
                    pretty_print=True,
                    xml_declaration=True,
                    encoding='utf-8'
                ).decode('utf-8')
            )
            pretty_xml_as_string = xml.toprettyxml(indent='  ')
            f.write(pretty_xml_as_string)
        else:
            f.write(
                etree.tostring(
                    law,
                    pretty_print=True,
                    xml_declaration=True,
                    encoding='utf-8'
                ).decode('utf-8')
            )


def process_law(law_id):

    stdout.write('Processing law nr. %s...' % law_id)
    stdout.flush()

    (law_num, law_year) = law_id.split('/')
    law_num = int(law_num)
    law_year = int(law_year)

    stdout.write('cleaning...')
    stdout.flush()
    clean_law(law_num, law_year)

    stdout.write('making XML...')
    stdout.flush()
    make_xml(law_num, law_year, CURRENT_PARLIAMENT_VERSION)

    stdout.write(' done\n')


def usage(exec_name, message=None):
    stderr.write('Usage: %s [law_number>/<year>] [year] [-a]\n' % exec_name)
    stderr.write('\n')
    stderr.write('Options:\n')
    stderr.write('    -a    Process all available laws.\n')
    stderr.write('\n')
    if message:
        stderr.write('Error: %s\n' % message)
    quit(1)


def main(argv):

    law_ids = set()
    for arg in argv:
        if arg == '-a':
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^\d{7}\.html$', filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{4}$', arg):
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^%s\d{3}\.html$' % arg, filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{1,4}\/\d{4}$', arg):
            law_ids.add(arg)

    if len(law_ids) == 0:
        usage(argv[0], 'No files matched the given criteria.')

    for law_id in law_ids:
        process_law(law_id)


try:
    main(sys.argv)
except KeyboardInterrupt:
    quit()
except Exception as e:
    if DEBUG:
        raise
    else:
        stderr.write('Error: %s\n' % e)
