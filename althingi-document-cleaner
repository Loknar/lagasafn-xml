#!/usr/bin/env python
# althingi-document-cleaner
# Version 0.1

from bs4 import BeautifulSoup
import codecs
import urllib
import re

import os
import sys

from copy import copy
from lxml import etree
from lxml.builder import E
from sys import stderr
from sys import stdout

DEBUG = True

CURRENT_PARLIAMENT_VERSION = '148a'

LAW_FILENAME = CURRENT_PARLIAMENT_VERSION + '/%d%s.html' # % (law_year, law_num)
CLEAN_FILENAME = 'cleaned/%d-%d.html' # % (law_year, law_num)
XML_FILENAME = 'xml/%d.%d.%s.xml' # % (law_year, law_num, parliament_version)

def clean_content(content):
    # Decode ISO-8859-1 character encoding.
    #content = content.decode('ISO-8859-1')

    # Replace HTML-specific space for normal space.
    content = content.replace('&nbsp;', ' ')

    # Make sure that horizontal bar tags are closed properly.
    content = content.replace('<hr>', '<hr />')

    # Make sure that linebreak tags are closed properly.
    content = content.replace('<br>', '<br />')

    # Remove markers for previous changes
    content = content.replace('[', '').replace(']', '')

    # Remove links to website
    #strings_to_remove = (
    #    u'Ferill málsins á Alþingi.',
    #    u'Frumvarp til laga.',
    #)
    #for s in strings_to_remove:
    #    content = content.replace(s, '')

    # Make sure that image tags are closed properly.
    e = re.compile(r'<img ([^>]*)>', re.IGNORECASE)
    content = e.sub(r'<img \1 />', content)

    # Remove <a id=""> tags which are unclosed and seem without purpose.
    # For example, see source of page: http://www.althingi.is/altext/143/s/0470.html
    content = content.replace('<a id="">', '')

    # Find the law content and exit if it does not exist
    soup_law = BeautifulSoup(content, 'html5lib').find('html')
    if soup_law is None:
        return None

    soup = BeautifulSoup(soup_law.__str__(), 'html5lib') # Parse the law content

    # Remove superscripts indicating previous change
    superscripts = soup.find_all('sup')
    for s in superscripts:
        if re.match('^\d{1,3}\)$', s.text):
            s.extract()

    # Remove links
    #anchors = soup.find_all('a')
    #for a in anchors:
    #    a.replace_with(a.text)

    # Remove tags entirely irrelevant to content
    tags_to_remove = ['small'] # Previously also ['hr', 'script', 'noscript', 'head']
    for target_tag in tags_to_remove:
        [s.extract() for s in soup(target_tag)]

    # Remove empty tags, but only if they're empty
    empty_tags_to_remove = ['p', 'h2', 'i']
    for target_tag in empty_tags_to_remove:
        empty_tags = soup.find_all(lambda tag: tag.name == target_tag and not tag.contents and (
            tag.string is None or not tag.string.strip()
        ))
        [empty_tag.extract() for empty_tag in empty_tags]

    # Keep consecutive <br />s only at 2 at most
    brs_in_a_row = 0
    all_tags = soup.find_all()
    for t in all_tags:
        if t.name == 'br':
            if brs_in_a_row >= 2:
                t.extract()
            else:
                brs_in_a_row = brs_in_a_row + 1
        else:
            brs_in_a_row = 0

    # Replace <html> and <body> tags' with <div>s.
    '''
    body_tag = soup.find('body')
    if body_tag is not None:
        body_tag.attrs['id'] = 'body_tag'
        body_tag.name = 'div'
    html_tag = soup.find('html')
    html_tag.attrs['id'] = 'html_tag'
    html_tag.name = 'div'
    '''

    # Add charset tag
    charset_tag = soup.new_tag('meta', charset='utf-8')
    soup.insert(0, charset_tag)

    xhtml = soup.prettify()

    # Final cleanup at text-stage.
    xhtml = xhtml.replace(' <!-- Tab -->\n  ', '&nbsp;&nbsp;&nbsp;&nbsp;')

    return xhtml


def clean_law(law_num, law_year):
    with codecs.open(LAW_FILENAME % (law_year, str(law_num).zfill(3)), 'r', 'ISO-8859-1') as infile:
        raw_content = infile.read()
        infile.close()

    content = clean_content(raw_content)

    if content is None:
        stdout.write(' failed.\n')
        stderr.write('Error: Law %d/%d does not seem to exist\n' % (law_year, law_num))
        quit(1)

    if not os.path.isdir(os.path.dirname(CLEAN_FILENAME)):
        os.mkdir(os.path.dirname(CLEAN_FILENAME))

    with open(CLEAN_FILENAME % (law_year, law_num), 'w') as clean_file:
        clean_file.write(content)
        clean_file.close()


def make_xml(law_num, law_year, parliament_version):

    # A super-iterator for containing all sorts of extra functionality that we
    # dont' get with a regular Python iterator. Note that this thing is
    # incompatible with yields and is NOT a subclass of `iter` (since that's
    # not possible), but rather a class trying its best to masquerade as one.
    #
    # TODO: See if the collection functions below rather belong in here.
    class super_iter():
        def __init__(self, collection):
            self.collection = collection
            self.index = 0

        def __next__(self):
            try:
                result = self.collection[self.index]
                self.index += 1
            except IndexError:
                raise StopIteration
            return result

        '''
        def prev(self):
            self.index -= 1
            if self.index < 0:
                raise StopIteration
            return self.collection[self.index]
        '''

        def __iter__(self):
            return self

        # Peek into the next item of the iterator without advancing it.
        # Works with negative numbers to take a peek at previous items.
        def peek(self, number_of_lines=1):
            peek_index = self.index - 1 + number_of_lines
            if peek_index >= len(self.collection) or peek_index < 0:
                return None
            return self.collection[peek_index]

    # Make sure that the XML output directory exists.
    if not os.path.isdir(os.path.dirname(XML_FILENAME)):
        os.mkdir(os.path.dirname(XML_FILENAME))

    # Construct the output XML object.
    law = E.law('', {'nr': str(law_num), 'year': str(law_year), 'parliament': parliament_version})

    # Open and read the cleaned HTML version of the law.
    with open(CLEAN_FILENAME % (law_year, law_num)) as clean_file:
        lines = super_iter(clean_file.readlines())
        clean_file.close()

    # Keeps track of the turn of events. We can query this list to check for
    # example whether the name of the document has been processed, or what the
    # last thing to be processed was. This gives us context when determining
    # what to do next.
    trail = ['start']

    # A helper class to be able to check if a regex matches in an
    # if-statement, but then process the results in its body, if there's a
    # match. This is essentially to make up for Python's (consciously decided)
    # inability to assign values to variables inside if-statements. Note that
    # a single instance of it is created and then used repeatedly.
    #
    # Usage:
    #
    # if matcher.check(line, '<tag goo="(\d+)" splah="(\d+)">'):
    #     goo, splah = matcher.result()
    #
    class Matcher():
        match = None
        def check(self, line, test_string):
            self.match = re.match(test_string, line)
            return self.match != None

        def result(self):
            return self.match.groups()
    matcher = Matcher()

    # Functions for collecting heaps of text, then returning it all in one go
    # and resetting the collection for the next time we need to collect a
    # bunch of text.
    def collect(string):
        if not hasattr(collect, 'collection'):
            collect.collection = []
        collect.collection.append(string.strip())
    def uncollect():
        result = ' '.join(collect.collection).strip()
        collect.collection = []
        return result

    # Will collect lines until the given string is found, and then return the
    # collected lines.
    def collect_until(lines, end_string):
        done = False
        while not done:
            line = next(lines).strip()
            if matcher.check(line, end_string):
                done = True
                continue
            collect(line)

        total = uncollect().strip()

        return total

    # Scrolls the lines until the given string is found. It works internally
    # the same as the collect_until-function, but is provided here with a
    # different name to provide a semantic distinction in the code below.
    scroll_until = collect_until

    # Iterators for keeping track of the order of things.
    chapter_i = 0

    # Objects that help us figure out the current state of affairs. These
    # variables are used between iterations, meaning that whenever possible,
    # their values should make sense at the end of the processing of a
    # particular line or clause. Never put nonsense into them because it will
    # completely confuse the processing elsewhere.
    chapter = None
    art = None
    subart = None

    # The cleaned document that we're processing is expected to put every tag,
    # both its opening and closing, on a separate line. This allows us to
    # browse the HTML contents on a per-line basis.
    for line in lines:
        line = line.strip()

        if line == '<h2>':
            # Parse law name.
            name = collect_until(lines, '</h2>')

            law.append(E.name(name))

            trail.append('name')

        elif line == '<strong>':
            if trail[-1] == 'name':
                # Parse the num and date, which appears directly below the law name.
                num_and_date = collect_until(lines, '</strong>')

                law.append(E('num-and-date', num_and_date))

                trail.append('num-and-date')

        elif line == '<hr/>':
            if trail[-1] == 'num-and-date':
                # Parse the whole clause about which minister the law refers
                # to. It contains HTML goo, but we'll just let it float along.
                # It's not even certain that we'll be using it, but there's no
                # harm in keeping it around.
                minister_clause = collect_until(lines, '<hr/>')

                law.append(E('minister-clause', minister_clause))

                trail.append('minister-clause')

        elif line == '<b>':
            # Chapters are found by finding a <b> tag that comes right after a
            # <br/> tag that occurs after the ministerial clause is done.
            if 'minister-clause' in trail and lines.peek(-1).strip() == '<br/>':
                # Parse what we will believe to be a chapter.

                # Chapter names are a bit tricky. They may be divided into two
                # <b> clauses, in which case the former one is what we call a
                # nr-title (I. kafli, II. kafli etc.), and then a name
                # describing its content (Almenn ákvæði, Tilgangur og markmið
                # etc.).
                #
                # Sometimes however, there is only one <b> and not two. In
                # these cases, there is no nr-title and only a name. Here we
                # check 3 lines into the future to see if there's another <b>
                # tag coming (because we know that the former <b>text</b>
                # clause will be precisely 3 lines), and if so, we'll process
                # the nr-title and move on. The next <b>, containing the name,
                # will be caught in the next iteration). If there is no such
                # impending tag, we'll interpret the entire contents of this
                # <b> as the name of the chapter and be done with it.
                #
                # Note in particular, the difference in trails that we leave.
                if lines.peek(3).strip() == '<b>':
                    chapter_nr_title = collect_until(lines, '</b>')

                    chapter_i += 1
                    chapter = E.chapter(
                        {'nr': str(chapter_i)},
                        E('nr-title', chapter_nr_title)
                    )

                    trail.append('chapter-nr-title')

                else:
                    chapter_name = collect_until(lines, '</b>')

                    chapter = E.chapter(
                        E('name', chapter_name)
                    )

                    trail.append('chapter-name')

                law.append(chapter)

            elif trail[-1] == 'chapter-nr-title':
                # Parse the name of the chapter. This only happens if the
                # chapter name is split into two distinct <b> tags, separated
                # into nr-title and name. See comment above for details.
                chapter_name = collect_until(lines, '</b>')

                chapter.append(E('name', chapter_name))

                trail.append('chapter-name')

        elif matcher.check(line, '<span id="G(\d+)">'):
            # Parse an article.
            art_nr = matcher.result()[0]

            art = E('art', {'nr': art_nr })
            chapter.append(art)

            scroll_until(lines, '<b>')
            art_nr_title = collect_until(lines, '</b>')

            art.append(E('nr-title', art_nr_title))

            # Check if the next line is an <em>, because if so, then the
            # article has a name title and we need to grab it. Note that not
            # all articles have names.
            if lines.peek().strip() == '<em>':
                scroll_until(lines, '<em>')
                art_name = collect_until(lines, '</em>')

                art.append(E('name', art_name))

            trail.append('art-nr')

            # There can be no current subarticle if we've just discovered a
            # new article.
            subart = None

        elif matcher.check(line, '<img alt="" height="11" id="G(\d+)M(\d+)" src="\/lagas\/hk.jpg" width="11"\/>'):
            art_nr, subart_nr = matcher.result()

            # TODO: This content needs to be separated down further, but it's
            # a bit of an independent science that we will deal with
            # independently.
            content = collect_until(lines, '<br/>')

            subart = E('subart', {'nr': subart_nr }, E('sen', content))
            art.append(subart)

            trail.append('subart')

        elif matcher.check(line, '<span id="G(\d+)([0-9A-Z]*)L(\d+)">'):
            art_nr, goo, fake_numart_nr = matcher.result()

            # fake_numart_nr is named so because it doesn't give the current
            # numart_nr. Not only is it always a digit, when in fact a real
            # numart_nr can be alphabetical, but even if we only had numbers,
            # they would still be wrong in some recursive lists. This is why
            # we won't trust it, and will rather analyze the content of the
            # next line, which will contain the correct numart_nr in the
            # correct type.
            numart_nr = lines.peek().strip().strip('.')

            # Which entity will we append this numerical article to? We don't
            # really know, because it could be an article or a subarticle, so
            # we'll simply have to check. In any case, the entity that we'll
            # append the numerical article to will simply be called "parent".
            if subart is not None:
                parent = subart
            else:
                # We sure as $h17 assume "art" exists.
                parent = art

            # Create numerical article.
            numart = E('numart', {'nr': numart_nr })

            # Add the numerical article to its parent.
            parent.append(numart)

            numart_nr_title = collect_until(lines, '</span>')
            numart.append(E('nr-title', numart_nr_title))
            if not numart_nr_title.strip('.').isdigit():
                numart.attrib.update({'type': 'alphabet'})

            if lines.peek().strip() == '<i>':
                # Looks like this numerical article has a name.
                scroll_until(lines, '<i>')
                numart_name = collect_until(lines, '</i>')

                numart.append(E('name', numart_name))

            content = collect_until(lines, '<br/>')

            numart.append(E('sen', content))

            trail.append('numart')


    # Write the XML object to output file.
    with open(XML_FILENAME % (law_year, law_num, parliament_version), 'w') as f:
        # Importing a completely different XML library than the one we're
        # using elsewhere in the code is a bit weird, but this is the only one
        # we could find that does pretty printing with proper indenting. That
        # happens to be very important for seeing whether the end result
        # works. Since it's only used when DEBUG=True and is very much an
        # anomaly in the code, it is imported here instead of at the top of
        # the file.
        #
        # When not in DEBUG mode, we'll skip those shenanigans and write it
        # out with the same library as the one we use elsewhere.
        if DEBUG:
            import xml.dom.minidom
            xml = xml.dom.minidom.parseString(etree.tostring(
                    law,
                    pretty_print=True,
                    xml_declaration=True,
                    encoding='utf-8'
                ).decode('utf-8')
            )
            pretty_xml_as_string = xml.toprettyxml(indent='  ')
            f.write(pretty_xml_as_string)
        else:
            f.write(
                etree.tostring(
                    law,
                    pretty_print=True,
                    xml_declaration=True,
                    encoding='utf-8'
                ).decode('utf-8')
            )


def process_law(law_id):

    stdout.write('Processing law nr. %s...' % law_id)
    stdout.flush()

    (law_num, law_year) = law_id.split('/')
    law_num = int(law_num)
    law_year = int(law_year)

    stdout.write('cleaning...')
    stdout.flush()
    clean_law(law_num, law_year)

    stdout.write('making XML...')
    stdout.flush()
    make_xml(law_num, law_year, CURRENT_PARLIAMENT_VERSION)

    stdout.write(' done\n')


def usage(exec_name, message=None):
    stderr.write('Usage: %s [law_number>/<year>] [year] [-a]\n' % exec_name)
    stderr.write('\n')
    stderr.write('Options:\n')
    stderr.write('    -a    Process all available laws.\n')
    stderr.write('\n')
    if message:
        stderr.write('Error: %s\n' % message)
    quit(1)


def main(argv):

    law_ids = set()
    for arg in argv:
        if arg == '-a':
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^\d{7}\.html$', filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{4}$', arg):
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^%s\d{3}\.html$' % arg, filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{1,4}\/\d{4}$', arg):
            law_ids.add(arg)

    if len(law_ids) == 0:
        usage(argv[0], 'No files matched the given criteria.')

    for law_id in law_ids:
        process_law(law_id)


try:
    main(sys.argv)
except KeyboardInterrupt:
    quit()
except Exception as e:
    if DEBUG:
        raise
    else:
        stderr.write('Error: %s\n' % e)
