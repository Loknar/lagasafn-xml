#!/usr/bin/env python
# althingi-document-cleaner
# Version 0.1

from bs4 import BeautifulSoup
import codecs
import urllib
import re
import roman

import os
import sys

from copy import copy
from lxml import etree
from lxml.builder import E
from sys import stderr
from sys import stdout

DEBUG = True

CURRENT_PARLIAMENT_VERSION = '148a'

LAW_FILENAME = CURRENT_PARLIAMENT_VERSION + '/%d%s.html' # % (law_year, law_num)
CLEAN_FILENAME = 'cleaned/%d-%d.html' # % (law_year, law_num)
XML_FILENAME = 'xml/%d.%d.%s.xml' # % (law_year, law_num, parliament_version)

def clean_content(content):
    # Decode ISO-8859-1 character encoding.
    #content = content.decode('ISO-8859-1')

    # Replace HTML-specific space for normal space.
    content = content.replace('&nbsp;', ' ')

    # Make sure that horizontal bar tags are closed properly.
    content = content.replace('<hr>', '<hr />')

    # Make sure that linebreak tags are closed properly.
    content = content.replace('<br>', '<br />')

    # Remove markers for previous changes
    content = content.replace('[', '').replace(']', '')

    # Remove markers for removed content
    content = content.replace('…', '').replace('&hellip;', '')

    # Remove links to website
    #strings_to_remove = (
    #    u'Ferill málsins á Alþingi.',
    #    u'Frumvarp til laga.',
    #)
    #for s in strings_to_remove:
    #    content = content.replace(s, '')

    # Make sure that image tags are closed properly.
    e = re.compile(r'<img ([^>]*)>', re.IGNORECASE)
    content = e.sub(r'<img \1 />', content)

    # Remove <a id=""> tags which are unclosed and seem without purpose.
    # For example, see source of page: http://www.althingi.is/altext/143/s/0470.html
    content = content.replace('<a id="">', '')

    # Find the law content and exit if it does not exist
    soup_law = BeautifulSoup(content, 'html5lib').find('html')
    if soup_law is None:
        return None

    soup = BeautifulSoup(soup_law.__str__(), 'html5lib') # Parse the law content

    # Remove superscripts indicating previous change. Only removes them when
    # they are found outside of footnotes.
    superscripts = soup.find_all('sup')
    for s in superscripts:
        if s.parent.name != 'small' and re.match('^\d{1,3}\)$', s.text):
            s.extract()

    # Remove links
    #anchors = soup.find_all('a')
    #for a in anchors:
    #    a.replace_with(a.text)

    # Remove tags entirely irrelevant to content
    #tags_to_remove = ['small'] # Previously also ['hr', 'script', 'noscript', 'head']
    #for target_tag in tags_to_remove:
    #    [s.extract() for s in soup(target_tag)]

    # Remove empty tags, but only if they're empty
    empty_tags_to_remove = ['p', 'h2', 'i']
    for target_tag in empty_tags_to_remove:
        empty_tags = soup.find_all(lambda tag: tag.name == target_tag and not tag.contents and (
            tag.string is None or not tag.string.strip()
        ))
        [empty_tag.extract() for empty_tag in empty_tags]

    # Keep consecutive <br />s only at 2 at most
    brs_in_a_row = 0
    all_tags = soup.find_all()
    for t in all_tags:
        if t.name == 'br':
            if brs_in_a_row >= 2:
                t.extract()
            else:
                brs_in_a_row = brs_in_a_row + 1
        else:
            brs_in_a_row = 0

    # Replace <html> and <body> tags' with <div>s.
    '''
    body_tag = soup.find('body')
    if body_tag is not None:
        body_tag.attrs['id'] = 'body_tag'
        body_tag.name = 'div'
    html_tag = soup.find('html')
    html_tag.attrs['id'] = 'html_tag'
    html_tag.name = 'div'
    '''

    # Add charset tag
    charset_tag = soup.new_tag('meta', charset='utf-8')
    soup.insert(0, charset_tag)

    xhtml = soup.prettify()

    # Final cleanup at text-stage.
    xhtml = xhtml.replace(' <!-- Tab -->\n  ', '&nbsp;&nbsp;&nbsp;&nbsp;')

    return xhtml


def clean_law(law_num, law_year):
    with codecs.open(LAW_FILENAME % (law_year, str(law_num).zfill(3)), 'r', 'ISO-8859-1') as infile:
        raw_content = infile.read()
        infile.close()

    content = clean_content(raw_content)

    if content is None:
        stdout.write(' failed.\n')
        stderr.write('Error: Law %d/%d does not seem to exist\n' % (law_year, law_num))
        quit(1)

    if not os.path.isdir(os.path.dirname(CLEAN_FILENAME)):
        os.mkdir(os.path.dirname(CLEAN_FILENAME))

    with open(CLEAN_FILENAME % (law_year, law_num), 'w') as clean_file:
        clean_file.write(content)
        clean_file.close()


def make_xml(law_num, law_year, parliament_version):

    # A super-iterator for containing all sorts of extra functionality that we
    # dont' get with a regular Python iterator. Note that this thing is
    # incompatible with yields and is NOT a subclass of `iter` (since that's
    # not possible), but rather a class trying its best to masquerade as one.
    #
    # TODO: See if the collection functions below rather belong in here.
    class super_iter():
        def __init__(self, collection):
            self.collection = collection
            self.index = 0

        def __next__(self):
            try:
                result = self.collection[self.index]
                self.index += 1
            except IndexError:
                raise StopIteration
            return result

        '''
        def prev(self):
            self.index -= 1
            if self.index < 0:
                raise StopIteration
            return self.collection[self.index]
        '''

        def __iter__(self):
            return self

        # Peek into the next item of the iterator without advancing it.
        # Works with negative numbers to take a peek at previous items.
        def peek(self, number_of_lines=1):
            peek_index = self.index - 1 + number_of_lines
            if peek_index >= len(self.collection) or peek_index < 0:
                return None
            return self.collection[peek_index]

    # Make sure that the XML output directory exists.
    if not os.path.isdir(os.path.dirname(XML_FILENAME)):
        os.mkdir(os.path.dirname(XML_FILENAME))

    # Construct the output XML object.
    law = E.law('', {'nr': str(law_num), 'year': str(law_year), 'parliament': parliament_version})

    # Open and read the cleaned HTML version of the law.
    with open(CLEAN_FILENAME % (law_year, law_num)) as clean_file:
        lines = super_iter(clean_file.readlines())
        clean_file.close()

    # Keeps track of the turn of events. We can query this list to check for
    # example whether the name of the document has been processed, or what the
    # last thing to be processed was. This gives us context when determining
    # what to do next.
    trail = ['start']

    # A helper class to be able to check if a regex matches in an
    # if-statement, but then process the results in its body, if there's a
    # match. This is essentially to make up for Python's (consciously decided)
    # inability to assign values to variables inside if-statements. Note that
    # a single instance of it is created and then used repeatedly.
    #
    # Usage:
    #
    # if matcher.check(line, '<tag goo="(\d+)" splah="(\d+)">'):
    #     goo, splah = matcher.result()
    #
    class Matcher():
        match = None
        def check(self, line, test_string):
            self.match = re.match(test_string, line)
            return self.match != None

        def result(self):
            return self.match.groups()
    matcher = Matcher()

    # Functions for collecting heaps of text, then returning it all in one go
    # and resetting the collection for the next time we need to collect a
    # bunch of text.
    def collect(string):
        if not hasattr(collect, 'collection'):
            collect.collection = []
        collect.collection.append(string.strip())
    def uncollect():
        result = ' '.join(collect.collection).strip()
        collect.collection = []
        return result

    # Will collect lines until the given string is found, and then return the
    # collected lines.
    def collect_until(lines, end_string):
        done = False
        while not done:
            line = next(lines).strip()
            if matcher.check(line, end_string):
                done = True
                continue
            collect(line)

        total = uncollect().strip()

        return total

    # A function for intelligently splitting textual content into separate
    # sentences based on linguistic rules and patterns.
    def separate_sentences(content):

        # Contains the resulting list of sentences.
        sens = []

        # Reference shorthands are strings that are used in references. They
        # are often combined to designate a particular location in legal text,
        # for example "7. tölul. 2. mgr. 5. gr.", meaning numerical article 7
        # in subarticle 2 of article 5. Their use results in various
        # combinations of numbers and dots that need to be taken into account
        # to avoid wrongly starting a new sentence when they are encountered.
        reference_shorthands = ['gr', 'mgr', 'málsl', 'tölul', 'staf']

        # Encode recognized short-hands in text so that the dots in them don't
        # get confused for an end of a sentence. They will be decoded when
        # appended to the resulting list.
        #
        # Note that whether there should be a dot at the end of these depends
        # on how they are typically used in text. Any of these that might be
        # used at the end of a sentence should preferably not have a dot at
        # the end. Those that are very unlikely to be used at the end of a
        # sentence should however and with a dot.
        #
        # This is because there is an ambiguity after one of these is used, if
        # the following letter is a capital letter, because the capital letter
        # may indicate either the start of a new sentence, OR it could just be
        # the name of something, since names start with capital letters. This
        # is why "a.m.k." and "þ.m.t." end with dots, because they very well
        # have a capitalized name after them but are very unlikely to be used
        # at the end of a sentence, while "o.fl." is extremely unlikely to be
        # followed by a name, but may very well end a sentence.
        recognized_shorts = [
            't.d.',
            'þ.m.t.',
            'sbr.',
            'nr.',
            'skv.',
            'm.a.',
            'a.m.k.',
            'þ.e.',
            'o.fl',
        ]
        for r in recognized_shorts:
            content = content.replace(r, r.replace('.', '[DOT]'))

        # The collected sentence so far. Chunks are appended to this string
        # until a new sentence is determined to be appropriate. Starts empty
        # and and is reset for every new sentence.
        collected = ''

        # We'll default to splitting chunks by dots. As we iterate through the
        # chunks, we will determine the cases where we actually don't want to
        # start a new sentence.
        chunks = super_iter(content.split('.'))

        for chunk in chunks:
            # There is usually a period at the en and therefore a trailing,
            # empty chunk that we're not interested in.
            if chunk == '':
                continue

            # Start a new sentence by default. We'll only continue to gather
            # the chunks into the collected sentence when we find a reason to,
            # but normally a dot means an end of a sentence, thus a new one.
            split = True

            # Collect the chunk into the sentence so far. If we decide not to
            # start a new sentence, then this variable will grow until we
            # decide to, at which point it's added to the result and cleared.
            collected += chunk

            # Previews the next chunk before it is processed itself, so that
            # we can determine the context in both directions.
            next_chunk = chunks.peek()

            if next_chunk is not None:
                # Don't start a new sentence if the first character in the
                # next chunk is lowercase.
                if len(next_chunk) > 1 and next_chunk[0] == ' ' and next_chunk[1].islower():
                    split = False

                # Don't start a new sentence if the character immediately
                # following the dot is a symbol indicating that the sentence's
                # end has not yet been reached (comma, semicomma etc.).
                if len(next_chunk)> 0 and next_chunk[0] in [',', ';', '–', '-']:
                    split = False

                # Don't split if dealing with a reference to an article,
                # sub-article, numerical article or whatever.
                # Example:
                #    3. mgr. 4. tölul. 1. gr.
                last_word = chunk[chunk.rfind(' ')+1:]
                if last_word in reference_shorthands:
                    next_chunk2 = chunks.peek(2)
                    if next_chunk.strip().isdigit() and next_chunk2.strip() in reference_shorthands:
                        split = False

            # Add the dot that we dropped when splitting.
            collected += '.'

            if split:
                # Decode the "[DOT]"s back into normal dots.
                collected = collected.replace('[DOT]', '.')

                # Append the collected sentence.
                sens.append(collected.strip())

                # Reset the collected sentence.
                collected = ''

        # Since we needed to drop the dot when splitting, we needed to add it
        # again to every chunk. Sometimes the content in its entirety doesn't
        # end with a dot though, but rather a comma or colon or some such
        # symbol. In these cases we have wrongly added it to the final chunk
        # after the split, and so we'll just remove it here. This could
        # probably be done somewhere inside the loop, but it would probably
        # just be less readable.
        if content and content[-1] != '.' and sens[-1][-1] == '.':
            sens[-1] = sens[-1].strip('.')

        return sens

    # Scrolls the lines until the given string is found. It works internally
    # the same as the collect_until-function, but is provided here with a
    # different name to provide a semantic distinction in the code below.
    scroll_until = collect_until

    # Objects that help us figure out the current state of affairs. These
    # variables are used between iterations, meaning that whenever possible,
    # their values should make sense at the end of the processing of a
    # particular line or clause. Never put nonsense into them because it will
    # completely confuse the processing elsewhere.
    chapter = None
    art = None
    subart = None

    # The cleaned document that we're processing is expected to put every tag,
    # both its opening and closing, on a separate line. This allows us to
    # browse the HTML contents on a per-line basis.
    for line in lines:
        line = line.strip()

        if line == '<h2>':
            # Parse law name.
            name = collect_until(lines, '</h2>')

            law.append(E.name(name))

            trail.append('name')

        elif line == '<strong>':
            if trail[-1] == 'name':
                # Parse the num and date, which appears directly below the law name.
                num_and_date = collect_until(lines, '</strong>')

                law.append(E('num-and-date', num_and_date))

                trail.append('num-and-date')

        elif line == '<hr/>':
            if trail[-1] == 'num-and-date':
                # Parse the whole clause about which minister the law refers
                # to. It contains HTML goo, but we'll just let it float along.
                # It's not even certain that we'll be using it, but there's no
                # harm in keeping it around.
                minister_clause = collect_until(lines, '<hr/>')

                law.append(E('minister-clause', minister_clause))

                trail.append('minister-clause')

        elif line == '<b>':
            # Chapters are found by finding a <b> tag that comes right after a
            # <br/> tag that occurs after the ministerial clause is done.
            if 'minister-clause' in trail and lines.peek(-1).strip() == '<br/>':
                # Parse what we will believe to be a chapter.

                # Chapter names are a bit tricky. They may be divided into two
                # <b> clauses, in which case the former one is what we call a
                # nr-title (I. kafli, II. kafli etc.), and then a name
                # describing its content (Almenn ákvæði, Tilgangur og markmið
                # etc.).
                #
                # Sometimes however, there is only one <b> and not two. In
                # these cases, there is no nr-title and only a name. Here we
                # check 3 lines into the future to see if there's another <b>
                # tag coming (because we know that the former <b>text</b>
                # clause will be precisely 3 lines), and if so, we'll process
                # the nr-title and name, but otherwise we will only process
                # the name and skip nr-title altogether.
                if lines.peek(3).strip() == '<b>':
                    chapter_nr_title = collect_until(lines, '</b>')

                    # Let's see if we can figure out the number of this
                    # chapter. We're going to assume the format "II. kafli"
                    # for the second chapter, and "II. kafli B." for the
                    # second chapter B, and in those examples, nr-titles will
                    # be "2" and "2b" respectively.
                    t = chapter_nr_title
                    nr = str(roman.fromRoman(t[0:t.index('.')]))
                    alpha = t[t.index('kafli') + 6:].strip('.')
                    if alpha:
                        nr += alpha.lower()
                    del t

                    scroll_until(lines, '<b>')
                    chapter_name = collect_until(lines, '</b>')

                    chapter = E.chapter(
                        {'nr': str(nr)},
                        E('nr-title', chapter_nr_title),
                        E('name', chapter_name)
                    )

                else:
                    chapter_name = collect_until(lines, '</b>')

                    chapter = E.chapter(
                        E('name', chapter_name)
                    )

                # Some laws have a chapter for temporary clauses, which may be
                # named something like "Bráðabirgðaákvæði", "Ákvæði til
                # bráðabirgða" and probably something else as well. We will
                # assume that a chapter name that includes that string
                # "bráðabirgð" is a chapter for temporary clauses and
                # furthermore that the string does NOT appear in any other
                # kind of chapter.
                if 'bráðabirgð' in chapter_name.lower():
                    chapter.attrib['type'] = 'temporary-clauses'

                law.append(chapter)

                trail.append('chapter')

        elif line == '<img alt="" height="11" src="/lagas/sk.jpg" width="11"/>':
            # Parse an article.
            scroll_until(lines, '<b>')
            art_nr_title = collect_until(lines, '</b>')

            # Hopefully this stays None. Not because anything will break
            # otherwise, but because Roman numerals suck ass.
            art_roman_nr = None

            # Analyze the displayed article name.
            try:
                # A typical article is called something like "13. gr." or
                # "13. gr. b". The common theme among typical articles is that
                # the string ". gr." will appear in them somewhere. Anything
                # before it is the article number. Anything after it, is an
                # extra thing which is appended to article names when new
                # articles are placed between two existing ones. For example,
                # if there already are articles 13 and 14 but the legislature
                # believes that a new article properly belongs between them,
                # the article will probably be called "13. gr. a". We would
                # want that translated into an sortable `art_nr` f.e. "13a".

                # Find index of common string. If the string does not exist,
                # it's some sort of a special article. A ValueError will be
                # raised and we'll deal with it below.
                gr_index = art_nr_title.index('. gr.')

                # Determine the numeric part of the article's name.
                art_nr = art_nr_title[0:gr_index]

                # Check if there is an extra part to the name which we'll want
                # appended to the `art_nr`.
                #
                # Note: len('.gr.') + 1 = 6
                art_nr_extra = art_nr_title[gr_index+6:].strip('.')
                if len(art_nr_extra):
                    art_nr = '%s%s' % (art_nr, art_nr_extra)

            except ValueError:
                # This means that the article's name is formatted in an
                # unconventional way. Typically this occurs only in temporary
                # clauses that tend to be numbered with some stupid fucking
                # Roman bullshit.
                art_nr = None
                try:
                    art_roman_nr = str(roman.fromRoman(art_nr_title.strip('.')))
                except roman.InvalidRomanNumeralError:
                    # Good. Fuck Roman numerals.
                    pass

            # Create the tags, configure them and append to the chapter.
            art = E('art', E('nr-title', art_nr_title))
            if art_nr is not None:
                art.attrib['nr'] = art_nr
            if art_roman_nr is not None:
                art.attrib['roman-nr'] = art_roman_nr
                art.attrib['number-type'] = 'roman'
            chapter.append(art)

            # Check if the next line is an <em>, because if so, then the
            # article has a name title and we need to grab it. Note that not
            # all articles have names.
            if lines.peek().strip() == '<em>':
                scroll_until(lines, '<em>')
                art_name = collect_until(lines, '</em>')

                art.append(E('name', art_name))

            trail.append('art-nr')

            # There can be no current subarticle if we've just discovered a
            # new article.
            subart = None

        elif matcher.check(line, '<img .+ id="[GB](\d+)[A-Z]?M(\d+)" src=".+\/hk.jpg" .+\/>'):
            art_nr, subart_nr = matcher.result()

            content = collect_until(lines, '<br/>')

            subart = E('subart', {'nr': subart_nr })
            for sen in separate_sentences(content):
                subart.append(E('sen', sen))
            art.append(subart)

            trail.append('subart')

        elif matcher.check(line, '<span id="G(\d+)([0-9A-Z]*)L(\d+)">'):
            # fake_numart_nr is named so because it doesn't give the current
            # numart_nr. Not only is it always a digit, when in fact a real
            # numart_nr can be alphabetical, but even if we only had numbers,
            # they would still be wrong in some recursive lists. This is why
            # we won't trust it, and will rather analyze the content of the
            # next line, which will contain the correct numart_nr in the
            # correct type.
            numart_nr = lines.peek().strip().strip('.')

            # Dictates where we will place this numart.
            parent = None

            if trail[-1] == 'numart':
                # `prev_numart` is only defined here for readability. `numart` hasn't
                # been set yet, so it's still equivalent to the previous
                # `numart`, so technically we could just use `numart` instead.
                # But `numart` is going to change later, so we'd rather be
                # clear on the code semantics when using `numart` but still
                # referring to the previous one.
                prev_numart = numart

                prev_numart_nr = prev_numart.attrib['nr']

                # Check if the type of the numart has changed from numeric to
                # alphabetic or vice versa.
                different_type = any([
                    prev_numart_nr.isdigit() and not numart_nr.isdigit(),
                    not prev_numart_nr.isdigit() and numart_nr.isdigit()
                ])

                if prev_numart_nr.isdigit():
                    expected_numart_nr = str(int(prev_numart_nr) + 1)
                else:
                    expected_numart_nr = chr(int(ord(prev_numart_nr)) + 1)

                if expected_numart_nr != numart_nr or different_type:
                    if numart_nr == 'a' or (numart_nr.isdigit() and int(numart_nr) == 1):
                        # A new list has started within the one we were
                        # already processing, which we can tell because there
                        # was a `numart` before this one, but this `numart`
                        # says it's at the beginning, being either 'a' or 1.
                        # In this case, we'll choose the previous `numart` as
                        # the parent, so that this new list will be inside the
                        # previous one.
                        parent = prev_numart
                    else:
                        # A different list is being handled now, but it's not
                        # starting at the beginning (is neither 'a' nor 1).
                        # This means that we've been dealing with a sub-list
                        # which has now finished, so we want to continue
                        # appending this `numart` to the parent of the parent
                        # of the list we've been working on recently, which is
                        # the same parent as the nodes that came before we
                        # started the sub-list.
                        parent = prev_numart.getparent().getparent()
                else:
                    # This `numart` is simply the next one, so we'll want to
                    # append it to whatever node that the previous `numart`
                    # was appended to.
                    parent = prev_numart.getparent()

            # A parent may already be set above if `numart` currently being
            # handled is not the first one in its parent article/subarticle.
            # However, if it is indeed the first one, we need to figure out
            # where to place it. It can be placed wither in a subarticle or in
            # some cases, directly into an article without a subarticle.
            if parent is None:
                if subart is not None:
                    parent = subart
                else:
                    # We sure as $h17 assume "art" exists.
                    parent = art

            # Create numerical article.
            numart = E('numart', {'nr': numart_nr })

            # Add the numerical article to its parent.
            parent.append(numart)

            numart_nr_title = collect_until(lines, '</span>')
            numart.append(E('nr-title', numart_nr_title))
            if not numart_nr_title.strip('.').isdigit():
                numart.attrib.update({'type': 'alphabet'})

            if lines.peek().strip() == '<i>':
                # Looks like this numerical article has a name.
                scroll_until(lines, '<i>')
                numart_name = collect_until(lines, '</i>')

                numart.append(E('name', numart_name))

            content = collect_until(lines, '<br/>')

            for sen in separate_sentences(content):
                numart.append(E('sen', sen))

            trail.append('numart')

        elif line == '<small>':
            if 'minister-clause' in trail:

                footnotes = E('footnotes')

                if trail[-1] == 'chapter':
                    chapter.append(footnotes)
                else:
                    art.append(footnotes)

                trail.append('footnotes')

        elif line == '<sup style="font-size:60%">':
            if trail[-1] in ['footnotes', 'footnote']:
                footnote_nr = collect_until(lines, '</sup>').strip(')')

                peek = lines.peek().strip()
                if matcher.check(peek, '<a href="(\/altext\/.*)">'):
                    # This is a footnote regarding a legal change.
                    href = 'https://www.althingi.is%s' % matcher.result()[0]

                    scroll_until(lines, '<a href="(\/altext\/.*)">')
                    footnote_sen = collect_until(lines, '</a>')

                    footnote = E('footnote', {'href': href}, E('footnote-sen', footnote_sen))

                    if matcher.check(footnote_sen, 'L\. (\d+)\/(\d{4}), (\d+)\. gr\.'):
                        fn_law_nr, fn_law_year, fn_art_nr = matcher.result()
                        footnote.attrib['nr'] = fn_law_nr
                        footnote.attrib['year'] = fn_law_year
                        footnote.attrib['art'] = fn_art_nr

                    footnotes.append(footnote)

                    trail.append('footnote')

                elif matcher.check(peek, '<!--a href="ekkitil/ekkitil.*"-->'):
                    scroll_until(lines, '<!--a href="ekkitil/ekkitil.*"-->')

                    # We will gather everything we run across into a string
                    # called `gathered`, until we run into a string that
                    # indicates that either this footnote section has ended,
                    # or we've run across a new footnote.
                    #
                    # Either one of the expected tags should absolutely
                    # appear, but even if they don't, this will still error
                    # out instead of looping forever, despite the "while True"
                    # condition, because "next(lines)" will eventually run out
                    # of lines.
                    gathered = ''
                    while True:
                        if lines.peek().strip() in ['</small>', '<sup style="font-size:60%">']:
                            break
                        else:
                            # The text we want is separated into lines with
                            # arbitrary indenting and HTML comments which will
                            # later be removed. As a result, meaningless
                            # whitespace is all over the place. To fix this,
                            # we'll remove whitespace from either side of the
                            # string, but add a space at the end, which will
                            # occasionally result in a double whitespace or a
                            # whitespace between tags and content. Those will
                            # be fixed later, resulting in a neat string
                            # without any unwanted whitespace.
                            gathered += next(lines).strip() + ' '

                    # Get rid of HTML comments.
                    gathered = re.sub(r'<!--.*?"-->', '', gathered)

                    # Get rid of remaining unwanted whitespace (see above).
                    gathered = gathered.replace('  ', ' ').replace('> ', '>').replace(' </', '</').strip()

                    # Create the footnote node with the gathered content.
                    footnote = E('footnote', E('footnote-sen', gathered))

                    # Append the footnote to the `footnotes` node which is
                    # expected to exist from an earlier iteration.
                    footnotes.append(footnote)

                    trail.append('footnote')


    # Write the XML object to output file.
    with open(XML_FILENAME % (law_year, law_num, parliament_version), 'w') as f:
        # Importing a completely different XML library than the one we're
        # using elsewhere in the code is a bit weird, but this is the only one
        # we could find that does pretty printing with proper indenting. That
        # happens to be very important for seeing whether the end result
        # works. Since it's only used when DEBUG=True and is very much an
        # anomaly in the code, it is imported here instead of at the top of
        # the file.
        #
        # When not in DEBUG mode, we'll skip those shenanigans and write it
        # out with the same library as the one we use elsewhere.
        if DEBUG:
            import xml.dom.minidom
            xml = xml.dom.minidom.parseString(etree.tostring(
                    law,
                    pretty_print=True,
                    xml_declaration=True,
                    encoding='utf-8'
                ).decode('utf-8')
            )
            pretty_xml_as_string = xml.toprettyxml(indent='  ')
            f.write(pretty_xml_as_string)
        else:
            f.write(
                etree.tostring(
                    law,
                    pretty_print=True,
                    xml_declaration=True,
                    encoding='utf-8'
                ).decode('utf-8')
            )


def process_law(law_id):

    stdout.write('Processing law nr. %s...' % law_id)
    stdout.flush()

    (law_num, law_year) = law_id.split('/')
    law_num = int(law_num)
    law_year = int(law_year)

    stdout.write('cleaning...')
    stdout.flush()
    clean_law(law_num, law_year)

    stdout.write('making XML...')
    stdout.flush()
    make_xml(law_num, law_year, CURRENT_PARLIAMENT_VERSION)

    stdout.write(' done\n')


def usage(exec_name, message=None):
    stderr.write('Usage: %s [law_number>/<year>] [year] [-a]\n' % exec_name)
    stderr.write('\n')
    stderr.write('Options:\n')
    stderr.write('    -a    Process all available laws.\n')
    stderr.write('\n')
    if message:
        stderr.write('Error: %s\n' % message)
    quit(1)


def main(argv):

    law_ids = set()
    for arg in argv:
        if arg == '-a':
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^\d{7}\.html$', filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{4}$', arg):
            for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
                if re.match('^%s\d{3}\.html$' % arg, filename):
                    law_year = int(filename[0:4])
                    law_num = int(filename[4:7])
                    law_id = '%d/%d' % (law_num, law_year)
                    law_ids.add(law_id)
        elif re.match('^\d{1,4}\/\d{4}$', arg):
            law_ids.add(arg)

    if len(law_ids) == 0:
        usage(argv[0], 'No files matched the given criteria.')

    for law_id in law_ids:
        process_law(law_id)


try:
    main(sys.argv)
except KeyboardInterrupt:
    quit()
except Exception as e:
    if DEBUG:
        raise
    else:
        stderr.write('Error: %s\n' % e)
